**We are actively organizing the training framework and code. Once the paper is reviewed, the code and model checkpoints will be released here immediately ğŸ«¡**

**æˆ‘ä»¬æ­£åœ¨ç§¯æçš„æ•´ç†è®­ç»ƒæ¡†æ¶å’Œä»£ç ï¼Œè®ºæ–‡ä¸€æ—¦å®Œæˆå®¡é˜…ï¼Œä»£ç å’Œæ¨¡å‹æ£€æŸ¥ç‚¹å°†ç«‹å³å…¬å¸ƒåœ¨æ­¤å¤„ğŸ«¡**

# LaST

English | [ç®€ä½“ä¸­æ–‡](docs/cn/README_CN.md)

This repository contains the code and models for our paper "LaST: A Transformer-based Network for Spatio-Temporal Predictive Learning with Dynamic Local Awareness". The implementation is based on PyTorch and PyTorch Lightning frameworks.

# Status ğŸ”¬:

Our paper has now entered the peer-review process. We have diligently completed the writing and undergone multiple rounds of revision. We will also be continually uploading and updating non-core modules of the code. The full code will, of course, be released upon publication. Stay tuned!ğŸ«¡

Expected Timeline:
- [x] [2024-11-13] Model Implementation
- [x] [2024-12-26] Experimental Results
- [x] [2025-02-17] Further Analysis and Paper Writing
- [x] [2025-05-27] Paper Writing and Revision
- [ ] [now] The paper is under review & We are organising and uploading partial code.
- [ ] Code Release


# 1. Quick Start ğŸ‡:
```shell
conda create -n LaST python=3.12
conda activate LaST

# Install the required packages
pip install lightning wandb opencv-python torchmetrics torchvision matplotlib rich ipykernel xarray netcdf4 cartopy
# pip install lightning wandb opencv-python torchmetrics torchvision matplotlib rich ipykernel xarray netcdf4 cartopy -i https://mirrors.aliyun.com/pypi/simple

# (Optional) For Jupter Notebook users, you can install the kernel with the following command:
python -m ipykernel install --user --name=last
```




# 2. Train ğŸ‹ï¸â€â™‚ï¸ :
![](/docs/figs/Table1.jpg)
Overview of the datasets employed in our experiments.


## 2.1. Download the dataset ğŸ—‚ï¸:

To make it easier for everyone, we have organized and uploaded some commonly used datasets to Google Drive and Baidu Drive. You can directly download these datasets or download them yourself as needed.

The code structure for the data part is as follows:

```text
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ __init__.py  # If you need to add your own dataset, you need to include it in the data_dict dictionary and the setup_data() function in this file
â”‚   â”œâ”€â”€ TaxiBJ
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ conf.yaml               # Configuration file containing dataset-related parameters
â”‚   â”‚   â”œâ”€â”€ dataset.npz             # This is the TaxiBJ dataset file
â”‚   â”‚   â””â”€â”€ TaxiBJDataModule.py     # Data processing file
...
```
For a complete explanation of the data module and instructions on how to train on your own datasets, [please click here](docs/en/data.md).


<summary>ğŸ“‚ Click to expand full dataset download table</summary>

| Dataset Name                                                               | Google Drive Link                       | Baidu Drive Link                                                    | Description                                               |
|----------------------------------------------------------------------------|-----------------------------------------|---------------------------------------------------------------------|-----------------------------------------------------------|
| [TaxiBJ](https://github.com/TolicWang/DeepST/tree/master/data/TaxiBJ)      | [Download](https://drive.google.com/file/d/1HDN_hF2pOP2JT97kB8VCREIfe5Z22Co-/view?usp=sharing)                            | [Download](https://pan.baidu.com/s/1VDHPuy61GGwqt05t4NVH8A?pwd=iSHU) | `data/TaxiBJ/dataset.npz`                                 |
| [Weather Bench](https://github.com/pangeo-data/WeatherBench)(T2m, Tcc, Rl) | |                                                                     | `data/WeatherBench/5_625/2_temperature/{xxx}.nc`          |
| [Human3.6M](http://vision.imar.ro/human3.6m/description.php)               |                                         |                                                                     | `data/Human/images`&`data/Human/images_txt`               |
| [CORAv2.0](https://mds.nmdis.org.cn/)(Ssh)                                 | -                                       | -                                                                   | Please apply for the dataset at https://mds.nmdis.org.cn. |



## 2.3. Training ğŸ‹ï¸â€â™‚ï¸:
We provide two main methods for training your model, along with a script example for sequential training:

### âœ… Method 1: Prepare Configuration Files


### âœ… Method 2: Use Command-Line Arguments


### ğŸ” Sequential Training Script
 




# Acknowledgements & References ğŸ”—:

1. ğŸ«¡ Our overall training framework is largely inspired by [OpenSTL](https://github.com/chengtan9907/OpenSTL), which we adapted and refactored to better align with the standard PyTorch Lightning usage paradigm.
2. ğŸ«¡ Our core ideas are also significantly influenced by [PredFormer](https://arxiv.org/abs/2410.04733).

# Citation ğŸ“š:

If you find this repository useful, please consider citing our paper (citation to be updated upon publication):
```bibtex
@ARTICLE{Liu2025LaST,
    title = {LaST: A Transformer-based Network for Spatio-Temporal Predictive Learning with Dynamic Local Awareness},
    author = {Zijian Liu, Yehao Wang, Zhuolin Li, Jie Yu, Chengci Wang, Zhiyu Liu, Shuai Zhang and Lingyu Xu},
    booktitile = {},
    note = {Under review},
    year={2025}
}
```



---

If you spot any issues or have improvement ideas, we sincerely appreciate you opening an issue or submitting a pull requestğŸ˜Š. 

å—å­¦è¯†æ‰€é™ï¼Œå¦‚æ‚¨å‘ç°ä»»ä½•é—®é¢˜æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼Œæ³è¯·åœ¨Issuesä¸­æå‡ºæˆ–ç›´æ¥æäº¤Pull Requestï¼Œæˆ‘ä»¬å°†ä¸èƒœæ„Ÿæ¿€å¹¶ç¬¬ä¸€æ—¶é—´å¤„ç†ğŸ˜Šã€‚



